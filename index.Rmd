---
title: "Weight lifting prediction"
author: "Claudio Lande"
date: "17/10/2019"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(corrplot)
library(rattle)
```

# 1 Executive Summary
This article shows how to create a classification model starting from the the data contained in the Weight Lifting Exercises Dataset (see [References]). The classification model is then used to predict the value of a variable in a different (test case) dataset.
In this article three different models will be created and compared. The best performing model will then be used to predict the values of variable _classe_ in the test case dataset.

# 2 Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# 3 Data loading and analysis
The first step consists in loading training and test case datasets.
```{r read_data}
set.seed(111)
trainDS <- read.csv("./pml-training.csv", stringsAsFactors = FALSE)
TestCaseSet <- read.csv("./pml-testing.csv", stringsAsFactors = FALSE)
dim(trainDS)
dim(TestCaseSet)
```
Analysing the data, one can see that many columns provide very little information as they contain mostly NA or are just empty.

Before trying to apply any regression or classification model it is important to have a complete dataset by removing the missing values or by imputing them. In this dataset, some columns have less than 5% of data and hence imputing missing data is not a recommendable option (more than 95% of the data should be estimated using less than 5% of it).

We decide to:

-- remove columns with less than 5% of valid values

-- convert column _classe_ (which contains the values to predict) from string to factor (with levels A to E)

-- remove first 7 columns which contain information not relevant for the classification (ID, timestamp, and the like)

As a last step the initial dataset is split into two groups, one for model training and the other for testing the model accuracy (we will choose the model that provides the highest accuracy).

```{r clean_data}

# Remove columns which contain almost only NA or are empty
trainDS$classe <- as.factor(trainDS$classe)

v <- sapply(trainDS, function(x) mean(!is.na(x))) > 0.95
trainDS <- trainDS[,v]
TestCaseSet  <- TestCaseSet[,v]
v <- sapply(names(trainDS), function(x) mean(trainDS[,x] != "") > 0.95)
trainDS <- trainDS[,v]
TestCaseSet  <- TestCaseSet[,v]

# Remove first seven columns which contain data unrelated to classification
trainDS <- trainDS[,-7:-1]
TestCaseSet  <- TestCaseSet[,-7:-1]

# create training and testing partitions dataset 
partition  <- createDataPartition(trainDS$classe, p=0.7, list=FALSE)
TrainSet <- trainDS[partition, ]
TestSet  <- trainDS[-partition, ]
dim(TrainSet)
```
After removing unnecessary data, the training set is left with 53 of the original 160 columns. Having a smaller dataset simplifies model construction and reduces computation time.

For sake of clarity, this experiment uses the following 3 datasets:

-- TrainSet: the training dataset which is used to create the classification models

-- TestSet: the dataset used for evaluating the accuracy of the classification models

-- TestCaseSet: this dataset contains data of 20 test cases. The best performing classification model will be used to predict the values of variable _classe_

## 3.1 Data correlation

The following diagram shows correlation among variables of the dataset (the darker the color, the stronger the correlation):
```{r data_correlation, fig.width=9, fig.height=9}
corMatrix <- cor(TrainSet[, -53])
corrplot(corMatrix, type = "upper", order = "hclust", method = "circle", tl.cex = 0.7,  tl.col="black", tl.srt=45)
```

# 4 Model creation and selection

## 4.1 Decision tree
The first model is based on a simple decision tree.
```{r decision_tree, cache=TRUE, fig.width=10, fig.height=6}
modelDecisionTree <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(modelDecisionTree)

# Use decision tree to predict results in test dataset
predictDecisionTree <- predict(modelDecisionTree, newdata=TestSet, type="class")
confMatDecisionTree <- confusionMatrix(predictDecisionTree, TestSet$classe)
accuracyDecisionTree <- round(confMatDecisionTree$overall['Accuracy'], 4)
confMatDecisionTree
```

The decision tree reaches the accuracy of (`r accuracyDecisionTree`) and as such the prediction contains a relatively high number of errors compared to the real values in the test set.


## 4.2 Random forest
Next we are going to create a new classification model based on the random forest algorithm. 
In order to avoid model overfitting, a k-fold (k = 3) cross-validation algorithm will be used. This way the model is constructed using different blocks of samples and hence it is expected to provide better perfomance when predicting values on a new dataset.
```{r random_forest, cache=TRUE}
ctrlRandomForest <- trainControl(method="cv", number=3, verboseIter=FALSE)
modelRandomForest <- train(classe ~ ., data=TrainSet, method="rf", trControl=ctrlRandomForest)
modelRandomForest$finalModel

# Apply model to estimate parameter classe in test data set
predictRandomForest <- predict(modelRandomForest, newdata=TestSet)

# Calculate accuracy
confMatRandomForest <- confusionMatrix(predictRandomForest, TestSet$classe)
accuracyRandomForest <- round(confMatRandomForest$overall['Accuracy'], 4)
confMatRandomForest

```

The model reaches an accuracy of (`r accuracyRandomForest`) which, as expected, is a very good improvement with respect to the decision tree model.
On the other hand models based on random forests require more computation time and are more difficult to interpret with respect to decision trees.

## 4.3 Generalized Boosted Model
Let's now create a generalized boosted model. This time we will use a repeated cross-validation technique in order to reduce model overfitting (3-fold cross-validation is performed 2 twice).
```{r generalized_boosted_model, cache=TRUE}
controlGBM  <- trainControl(method = "repeatedcv", number = 3, repeats = 2)
modelFitGBM <- train(classe ~ ., data=TrainSet, method = "gbm", trControl = controlGBM, verbose = FALSE)
modelFitGBM$finalModel

# Calculate accuracy
predictGBM <- predict(modelFitGBM, newdata = TestSet)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
accuracyGBM <- round(confMatGBM$overall['Accuracy'], 4)
confMatGBM
```
The generalized boosted model provides a very high accuracy with more that 96% of the samples correctly classified.

## 4.4 Model choice and predicion
Comparing the accuracy of the three models it is easy to observe that the random forest model is the one performing best on the test dataset:

| Model type                | Accuracy                 |
| --------------------------|--------------------------|
| Decision tree             | `r accuracyDecisionTree` |
| Random forest             | `r accuracyRandomForest` |
| Generalized Boosted Model | `r accuracyGBM`          |

Hence we will use the model base on random forest to estimate the values of the variable classe for the 20 samples contained in the testcase dataset.

```{r prediction, cache=TRUE}
predictClasseValuesRF <- predict(modelRandomForest, TestCaseSet)
predictClasseValuesRF

```

# References
[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.